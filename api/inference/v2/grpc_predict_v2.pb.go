// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: api/inference/v2/grpc_predict_v2.proto

package inference

import (
	context "context"
	fmt "fmt"
	proto "github.com/gogo/protobuf/proto"
	grpc "google.golang.org/grpc"
	math "math"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.GoGoProtoPackageIsVersion2 // please upgrade the proto package

type ServerLiveRequest struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerLiveRequest) Reset()         { *m = ServerLiveRequest{} }
func (m *ServerLiveRequest) String() string { return proto.CompactTextString(m) }
func (*ServerLiveRequest) ProtoMessage()    {}
func (*ServerLiveRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{0}
}
func (m *ServerLiveRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerLiveRequest.Unmarshal(m, b)
}
func (m *ServerLiveRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerLiveRequest.Marshal(b, m, deterministic)
}
func (m *ServerLiveRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerLiveRequest.Merge(m, src)
}
func (m *ServerLiveRequest) XXX_Size() int {
	return xxx_messageInfo_ServerLiveRequest.Size(m)
}
func (m *ServerLiveRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerLiveRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ServerLiveRequest proto.InternalMessageInfo

type ServerLiveResponse struct {
	// True if the inference server is live, false if not live.
	Live                 bool     `protobuf:"varint,1,opt,name=live,proto3" json:"live,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerLiveResponse) Reset()         { *m = ServerLiveResponse{} }
func (m *ServerLiveResponse) String() string { return proto.CompactTextString(m) }
func (*ServerLiveResponse) ProtoMessage()    {}
func (*ServerLiveResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{1}
}
func (m *ServerLiveResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerLiveResponse.Unmarshal(m, b)
}
func (m *ServerLiveResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerLiveResponse.Marshal(b, m, deterministic)
}
func (m *ServerLiveResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerLiveResponse.Merge(m, src)
}
func (m *ServerLiveResponse) XXX_Size() int {
	return xxx_messageInfo_ServerLiveResponse.Size(m)
}
func (m *ServerLiveResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerLiveResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ServerLiveResponse proto.InternalMessageInfo

func (m *ServerLiveResponse) GetLive() bool {
	if m != nil {
		return m.Live
	}
	return false
}

type ServerReadyRequest struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerReadyRequest) Reset()         { *m = ServerReadyRequest{} }
func (m *ServerReadyRequest) String() string { return proto.CompactTextString(m) }
func (*ServerReadyRequest) ProtoMessage()    {}
func (*ServerReadyRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{2}
}
func (m *ServerReadyRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerReadyRequest.Unmarshal(m, b)
}
func (m *ServerReadyRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerReadyRequest.Marshal(b, m, deterministic)
}
func (m *ServerReadyRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerReadyRequest.Merge(m, src)
}
func (m *ServerReadyRequest) XXX_Size() int {
	return xxx_messageInfo_ServerReadyRequest.Size(m)
}
func (m *ServerReadyRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerReadyRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ServerReadyRequest proto.InternalMessageInfo

type ServerReadyResponse struct {
	// True if the inference server is ready, false if not ready.
	Ready                bool     `protobuf:"varint,1,opt,name=ready,proto3" json:"ready,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerReadyResponse) Reset()         { *m = ServerReadyResponse{} }
func (m *ServerReadyResponse) String() string { return proto.CompactTextString(m) }
func (*ServerReadyResponse) ProtoMessage()    {}
func (*ServerReadyResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{3}
}
func (m *ServerReadyResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerReadyResponse.Unmarshal(m, b)
}
func (m *ServerReadyResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerReadyResponse.Marshal(b, m, deterministic)
}
func (m *ServerReadyResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerReadyResponse.Merge(m, src)
}
func (m *ServerReadyResponse) XXX_Size() int {
	return xxx_messageInfo_ServerReadyResponse.Size(m)
}
func (m *ServerReadyResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerReadyResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ServerReadyResponse proto.InternalMessageInfo

func (m *ServerReadyResponse) GetReady() bool {
	if m != nil {
		return m.Ready
	}
	return false
}

type ModelReadyRequest struct {
	// The name of the model to check for readiness.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The version of the model to check for readiness. If not given the
	// server will choose a version based on the model and internal policy.
	Version              string   `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelReadyRequest) Reset()         { *m = ModelReadyRequest{} }
func (m *ModelReadyRequest) String() string { return proto.CompactTextString(m) }
func (*ModelReadyRequest) ProtoMessage()    {}
func (*ModelReadyRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{4}
}
func (m *ModelReadyRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelReadyRequest.Unmarshal(m, b)
}
func (m *ModelReadyRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelReadyRequest.Marshal(b, m, deterministic)
}
func (m *ModelReadyRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelReadyRequest.Merge(m, src)
}
func (m *ModelReadyRequest) XXX_Size() int {
	return xxx_messageInfo_ModelReadyRequest.Size(m)
}
func (m *ModelReadyRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelReadyRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelReadyRequest proto.InternalMessageInfo

func (m *ModelReadyRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelReadyRequest) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

type ModelReadyResponse struct {
	// True if the model is ready, false if not ready.
	Ready                bool     `protobuf:"varint,1,opt,name=ready,proto3" json:"ready,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelReadyResponse) Reset()         { *m = ModelReadyResponse{} }
func (m *ModelReadyResponse) String() string { return proto.CompactTextString(m) }
func (*ModelReadyResponse) ProtoMessage()    {}
func (*ModelReadyResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{5}
}
func (m *ModelReadyResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelReadyResponse.Unmarshal(m, b)
}
func (m *ModelReadyResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelReadyResponse.Marshal(b, m, deterministic)
}
func (m *ModelReadyResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelReadyResponse.Merge(m, src)
}
func (m *ModelReadyResponse) XXX_Size() int {
	return xxx_messageInfo_ModelReadyResponse.Size(m)
}
func (m *ModelReadyResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelReadyResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelReadyResponse proto.InternalMessageInfo

func (m *ModelReadyResponse) GetReady() bool {
	if m != nil {
		return m.Ready
	}
	return false
}

type ServerMetadataRequest struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerMetadataRequest) Reset()         { *m = ServerMetadataRequest{} }
func (m *ServerMetadataRequest) String() string { return proto.CompactTextString(m) }
func (*ServerMetadataRequest) ProtoMessage()    {}
func (*ServerMetadataRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{6}
}
func (m *ServerMetadataRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerMetadataRequest.Unmarshal(m, b)
}
func (m *ServerMetadataRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerMetadataRequest.Marshal(b, m, deterministic)
}
func (m *ServerMetadataRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerMetadataRequest.Merge(m, src)
}
func (m *ServerMetadataRequest) XXX_Size() int {
	return xxx_messageInfo_ServerMetadataRequest.Size(m)
}
func (m *ServerMetadataRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerMetadataRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ServerMetadataRequest proto.InternalMessageInfo

type ServerMetadataResponse struct {
	// The server name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The server version.
	Version string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	// The extensions supported by the server.
	Extensions           []string `protobuf:"bytes,3,rep,name=extensions,proto3" json:"extensions,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerMetadataResponse) Reset()         { *m = ServerMetadataResponse{} }
func (m *ServerMetadataResponse) String() string { return proto.CompactTextString(m) }
func (*ServerMetadataResponse) ProtoMessage()    {}
func (*ServerMetadataResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{7}
}
func (m *ServerMetadataResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerMetadataResponse.Unmarshal(m, b)
}
func (m *ServerMetadataResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerMetadataResponse.Marshal(b, m, deterministic)
}
func (m *ServerMetadataResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerMetadataResponse.Merge(m, src)
}
func (m *ServerMetadataResponse) XXX_Size() int {
	return xxx_messageInfo_ServerMetadataResponse.Size(m)
}
func (m *ServerMetadataResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerMetadataResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ServerMetadataResponse proto.InternalMessageInfo

func (m *ServerMetadataResponse) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ServerMetadataResponse) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

func (m *ServerMetadataResponse) GetExtensions() []string {
	if m != nil {
		return m.Extensions
	}
	return nil
}

type ModelMetadataRequest struct {
	// The name of the model.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The version of the model to check for readiness. If not given the
	// server will choose a version based on the model and internal policy.
	Version              string   `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelMetadataRequest) Reset()         { *m = ModelMetadataRequest{} }
func (m *ModelMetadataRequest) String() string { return proto.CompactTextString(m) }
func (*ModelMetadataRequest) ProtoMessage()    {}
func (*ModelMetadataRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{8}
}
func (m *ModelMetadataRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelMetadataRequest.Unmarshal(m, b)
}
func (m *ModelMetadataRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelMetadataRequest.Marshal(b, m, deterministic)
}
func (m *ModelMetadataRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelMetadataRequest.Merge(m, src)
}
func (m *ModelMetadataRequest) XXX_Size() int {
	return xxx_messageInfo_ModelMetadataRequest.Size(m)
}
func (m *ModelMetadataRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelMetadataRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelMetadataRequest proto.InternalMessageInfo

func (m *ModelMetadataRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelMetadataRequest) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

type ModelMetadataResponse struct {
	// The model name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The versions of the model available on the server.
	Versions []string `protobuf:"bytes,2,rep,name=versions,proto3" json:"versions,omitempty"`
	// The model's platform. See Platforms.
	Platform string `protobuf:"bytes,3,opt,name=platform,proto3" json:"platform,omitempty"`
	// The model's inputs.
	Inputs []*ModelMetadataResponse_TensorMetadata `protobuf:"bytes,4,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// The model's outputs.
	Outputs              []*ModelMetadataResponse_TensorMetadata `protobuf:"bytes,5,rep,name=outputs,proto3" json:"outputs,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                                `json:"-"`
	XXX_unrecognized     []byte                                  `json:"-"`
	XXX_sizecache        int32                                   `json:"-"`
}

func (m *ModelMetadataResponse) Reset()         { *m = ModelMetadataResponse{} }
func (m *ModelMetadataResponse) String() string { return proto.CompactTextString(m) }
func (*ModelMetadataResponse) ProtoMessage()    {}
func (*ModelMetadataResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{9}
}
func (m *ModelMetadataResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelMetadataResponse.Unmarshal(m, b)
}
func (m *ModelMetadataResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelMetadataResponse.Marshal(b, m, deterministic)
}
func (m *ModelMetadataResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelMetadataResponse.Merge(m, src)
}
func (m *ModelMetadataResponse) XXX_Size() int {
	return xxx_messageInfo_ModelMetadataResponse.Size(m)
}
func (m *ModelMetadataResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelMetadataResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelMetadataResponse proto.InternalMessageInfo

func (m *ModelMetadataResponse) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelMetadataResponse) GetVersions() []string {
	if m != nil {
		return m.Versions
	}
	return nil
}

func (m *ModelMetadataResponse) GetPlatform() string {
	if m != nil {
		return m.Platform
	}
	return ""
}

func (m *ModelMetadataResponse) GetInputs() []*ModelMetadataResponse_TensorMetadata {
	if m != nil {
		return m.Inputs
	}
	return nil
}

func (m *ModelMetadataResponse) GetOutputs() []*ModelMetadataResponse_TensorMetadata {
	if m != nil {
		return m.Outputs
	}
	return nil
}

// Metadata for a tensor.
type ModelMetadataResponse_TensorMetadata struct {
	// The tensor name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The tensor data type.
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	// The tensor shape. A variable-size dimension is represented
	// by a -1 value.
	Shape                []int64  `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelMetadataResponse_TensorMetadata) Reset()         { *m = ModelMetadataResponse_TensorMetadata{} }
func (m *ModelMetadataResponse_TensorMetadata) String() string { return proto.CompactTextString(m) }
func (*ModelMetadataResponse_TensorMetadata) ProtoMessage()    {}
func (*ModelMetadataResponse_TensorMetadata) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{9, 0}
}
func (m *ModelMetadataResponse_TensorMetadata) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelMetadataResponse_TensorMetadata.Unmarshal(m, b)
}
func (m *ModelMetadataResponse_TensorMetadata) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelMetadataResponse_TensorMetadata.Marshal(b, m, deterministic)
}
func (m *ModelMetadataResponse_TensorMetadata) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelMetadataResponse_TensorMetadata.Merge(m, src)
}
func (m *ModelMetadataResponse_TensorMetadata) XXX_Size() int {
	return xxx_messageInfo_ModelMetadataResponse_TensorMetadata.Size(m)
}
func (m *ModelMetadataResponse_TensorMetadata) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelMetadataResponse_TensorMetadata.DiscardUnknown(m)
}

var xxx_messageInfo_ModelMetadataResponse_TensorMetadata proto.InternalMessageInfo

func (m *ModelMetadataResponse_TensorMetadata) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelMetadataResponse_TensorMetadata) GetDatatype() string {
	if m != nil {
		return m.Datatype
	}
	return ""
}

func (m *ModelMetadataResponse_TensorMetadata) GetShape() []int64 {
	if m != nil {
		return m.Shape
	}
	return nil
}

type ModelInferRequest struct {
	// The name of the model to use for inferencing.
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// The version of the model to use for inference. If not given the
	// server will choose a version based on the model and internal policy.
	ModelVersion string `protobuf:"bytes,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	// Optional identifier for the request. If specified will be
	// returned in the response.
	Id string `protobuf:"bytes,3,opt,name=id,proto3" json:"id,omitempty"`
	// Optional inference parameters.
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	// The input tensors for the inference.
	Inputs []*ModelInferRequest_InferInputTensor `protobuf:"bytes,5,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// The requested output tensors for the inference. Optional, if not
	// specified all outputs produced by the model will be returned.
	Outputs []*ModelInferRequest_InferRequestedOutputTensor `protobuf:"bytes,6,rep,name=outputs,proto3" json:"outputs,omitempty"`
	// The data contained in an input tensor can be represented in "raw"
	// bytes form or in the repeated type that matches the tensor's data
	// type. To use the raw representation 'raw_input_contents' must be
	// initialized with data for each tensor in the same order as
	// 'inputs'. For each tensor, the size of this content must match
	// what is expected by the tensor's shape and data type. The raw
	// data must be the flattened, one-dimensional, row-major order of
	// the tensor elements without any stride or padding between the
	// elements. Note that the FP16 and BF16 data types must be represented as
	// raw content as there is no specific data type for a 16-bit float type.
	//
	// If this field is specified then InferInputTensor::contents must
	// not be specified for any input tensor.
	RawInputContents     [][]byte `protobuf:"bytes,7,rep,name=raw_input_contents,json=rawInputContents,proto3" json:"raw_input_contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelInferRequest) Reset()         { *m = ModelInferRequest{} }
func (m *ModelInferRequest) String() string { return proto.CompactTextString(m) }
func (*ModelInferRequest) ProtoMessage()    {}
func (*ModelInferRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{10}
}
func (m *ModelInferRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferRequest.Unmarshal(m, b)
}
func (m *ModelInferRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferRequest.Marshal(b, m, deterministic)
}
func (m *ModelInferRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferRequest.Merge(m, src)
}
func (m *ModelInferRequest) XXX_Size() int {
	return xxx_messageInfo_ModelInferRequest.Size(m)
}
func (m *ModelInferRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferRequest proto.InternalMessageInfo

func (m *ModelInferRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *ModelInferRequest) GetModelVersion() string {
	if m != nil {
		return m.ModelVersion
	}
	return ""
}

func (m *ModelInferRequest) GetId() string {
	if m != nil {
		return m.Id
	}
	return ""
}

func (m *ModelInferRequest) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *ModelInferRequest) GetInputs() []*ModelInferRequest_InferInputTensor {
	if m != nil {
		return m.Inputs
	}
	return nil
}

func (m *ModelInferRequest) GetOutputs() []*ModelInferRequest_InferRequestedOutputTensor {
	if m != nil {
		return m.Outputs
	}
	return nil
}

func (m *ModelInferRequest) GetRawInputContents() [][]byte {
	if m != nil {
		return m.RawInputContents
	}
	return nil
}

// An input tensor for an inference request.
type ModelInferRequest_InferInputTensor struct {
	// The tensor name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The tensor data type.
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	// The tensor shape.
	Shape []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	// Optional inference input tensor parameters.
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	// The tensor contents using a data-type format. This field must
	// not be specified if "raw" tensor contents are being used for
	// the inference request.
	Contents             *InferTensorContents `protobuf:"bytes,5,opt,name=contents,proto3" json:"contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{}             `json:"-"`
	XXX_unrecognized     []byte               `json:"-"`
	XXX_sizecache        int32                `json:"-"`
}

func (m *ModelInferRequest_InferInputTensor) Reset()         { *m = ModelInferRequest_InferInputTensor{} }
func (m *ModelInferRequest_InferInputTensor) String() string { return proto.CompactTextString(m) }
func (*ModelInferRequest_InferInputTensor) ProtoMessage()    {}
func (*ModelInferRequest_InferInputTensor) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{10, 0}
}
func (m *ModelInferRequest_InferInputTensor) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferRequest_InferInputTensor.Unmarshal(m, b)
}
func (m *ModelInferRequest_InferInputTensor) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferRequest_InferInputTensor.Marshal(b, m, deterministic)
}
func (m *ModelInferRequest_InferInputTensor) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferRequest_InferInputTensor.Merge(m, src)
}
func (m *ModelInferRequest_InferInputTensor) XXX_Size() int {
	return xxx_messageInfo_ModelInferRequest_InferInputTensor.Size(m)
}
func (m *ModelInferRequest_InferInputTensor) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferRequest_InferInputTensor.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferRequest_InferInputTensor proto.InternalMessageInfo

func (m *ModelInferRequest_InferInputTensor) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInferRequest_InferInputTensor) GetDatatype() string {
	if m != nil {
		return m.Datatype
	}
	return ""
}

func (m *ModelInferRequest_InferInputTensor) GetShape() []int64 {
	if m != nil {
		return m.Shape
	}
	return nil
}

func (m *ModelInferRequest_InferInputTensor) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *ModelInferRequest_InferInputTensor) GetContents() *InferTensorContents {
	if m != nil {
		return m.Contents
	}
	return nil
}

// An output tensor requested for an inference request.
type ModelInferRequest_InferRequestedOutputTensor struct {
	// The tensor name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Optional requested output tensor parameters.
	Parameters           map[string]*InferParameter `protobuf:"bytes,2,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                   `json:"-"`
	XXX_unrecognized     []byte                     `json:"-"`
	XXX_sizecache        int32                      `json:"-"`
}

func (m *ModelInferRequest_InferRequestedOutputTensor) Reset() {
	*m = ModelInferRequest_InferRequestedOutputTensor{}
}
func (m *ModelInferRequest_InferRequestedOutputTensor) String() string {
	return proto.CompactTextString(m)
}
func (*ModelInferRequest_InferRequestedOutputTensor) ProtoMessage() {}
func (*ModelInferRequest_InferRequestedOutputTensor) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{10, 1}
}
func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.Unmarshal(m, b)
}
func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.Marshal(b, m, deterministic)
}
func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.Merge(m, src)
}
func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_Size() int {
	return xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.Size(m)
}
func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor proto.InternalMessageInfo

func (m *ModelInferRequest_InferRequestedOutputTensor) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInferRequest_InferRequestedOutputTensor) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

type ModelInferResponse struct {
	// The name of the model used for inference.
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// The version of the model used for inference.
	ModelVersion string `protobuf:"bytes,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	// The id of the inference request if one was specified.
	Id string `protobuf:"bytes,3,opt,name=id,proto3" json:"id,omitempty"`
	// Optional inference response parameters.
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	// The output tensors holding inference results.
	Outputs []*ModelInferResponse_InferOutputTensor `protobuf:"bytes,5,rep,name=outputs,proto3" json:"outputs,omitempty"`
	// The data contained in an output tensor can be represented in
	// "raw" bytes form or in the repeated type that matches the
	// tensor's data type. To use the raw representation 'raw_output_contents'
	// must be initialized with data for each tensor in the same order as
	// 'outputs'. For each tensor, the size of this content must match
	// what is expected by the tensor's shape and data type. The raw
	// data must be the flattened, one-dimensional, row-major order of
	// the tensor elements without any stride or padding between the
	// elements. Note that the FP16 and BF16 data types must be represented as
	// raw content as there is no specific data type for a 16-bit float type.
	//
	// If this field is specified then InferOutputTensor::contents must
	// not be specified for any output tensor.
	RawOutputContents    [][]byte `protobuf:"bytes,6,rep,name=raw_output_contents,json=rawOutputContents,proto3" json:"raw_output_contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelInferResponse) Reset()         { *m = ModelInferResponse{} }
func (m *ModelInferResponse) String() string { return proto.CompactTextString(m) }
func (*ModelInferResponse) ProtoMessage()    {}
func (*ModelInferResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{11}
}
func (m *ModelInferResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferResponse.Unmarshal(m, b)
}
func (m *ModelInferResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferResponse.Marshal(b, m, deterministic)
}
func (m *ModelInferResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferResponse.Merge(m, src)
}
func (m *ModelInferResponse) XXX_Size() int {
	return xxx_messageInfo_ModelInferResponse.Size(m)
}
func (m *ModelInferResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferResponse proto.InternalMessageInfo

func (m *ModelInferResponse) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *ModelInferResponse) GetModelVersion() string {
	if m != nil {
		return m.ModelVersion
	}
	return ""
}

func (m *ModelInferResponse) GetId() string {
	if m != nil {
		return m.Id
	}
	return ""
}

func (m *ModelInferResponse) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *ModelInferResponse) GetOutputs() []*ModelInferResponse_InferOutputTensor {
	if m != nil {
		return m.Outputs
	}
	return nil
}

func (m *ModelInferResponse) GetRawOutputContents() [][]byte {
	if m != nil {
		return m.RawOutputContents
	}
	return nil
}

// An output tensor returned for an inference request.
type ModelInferResponse_InferOutputTensor struct {
	// The tensor name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The tensor data type.
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	// The tensor shape.
	Shape []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	// Optional output tensor parameters.
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	// The tensor contents using a data-type format. This field must
	// not be specified if "raw" tensor contents are being used for
	// the inference response.
	Contents             *InferTensorContents `protobuf:"bytes,5,opt,name=contents,proto3" json:"contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{}             `json:"-"`
	XXX_unrecognized     []byte               `json:"-"`
	XXX_sizecache        int32                `json:"-"`
}

func (m *ModelInferResponse_InferOutputTensor) Reset()         { *m = ModelInferResponse_InferOutputTensor{} }
func (m *ModelInferResponse_InferOutputTensor) String() string { return proto.CompactTextString(m) }
func (*ModelInferResponse_InferOutputTensor) ProtoMessage()    {}
func (*ModelInferResponse_InferOutputTensor) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{11, 0}
}
func (m *ModelInferResponse_InferOutputTensor) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferResponse_InferOutputTensor.Unmarshal(m, b)
}
func (m *ModelInferResponse_InferOutputTensor) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferResponse_InferOutputTensor.Marshal(b, m, deterministic)
}
func (m *ModelInferResponse_InferOutputTensor) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferResponse_InferOutputTensor.Merge(m, src)
}
func (m *ModelInferResponse_InferOutputTensor) XXX_Size() int {
	return xxx_messageInfo_ModelInferResponse_InferOutputTensor.Size(m)
}
func (m *ModelInferResponse_InferOutputTensor) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferResponse_InferOutputTensor.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferResponse_InferOutputTensor proto.InternalMessageInfo

func (m *ModelInferResponse_InferOutputTensor) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInferResponse_InferOutputTensor) GetDatatype() string {
	if m != nil {
		return m.Datatype
	}
	return ""
}

func (m *ModelInferResponse_InferOutputTensor) GetShape() []int64 {
	if m != nil {
		return m.Shape
	}
	return nil
}

func (m *ModelInferResponse_InferOutputTensor) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *ModelInferResponse_InferOutputTensor) GetContents() *InferTensorContents {
	if m != nil {
		return m.Contents
	}
	return nil
}

// An inference parameter value. The Parameters message describes a
// “name”/”value” pair, where the “name” is the name of the parameter
// and the “value” is a boolean, integer, or string corresponding to
// the parameter.
type InferParameter struct {
	// The parameter value can be a string, an int64, a boolean
	// or a message specific to a predefined parameter.
	//
	// Types that are valid to be assigned to ParameterChoice:
	//
	//	*InferParameter_BoolParam
	//	*InferParameter_Int64Param
	//	*InferParameter_StringParam
	ParameterChoice      isInferParameter_ParameterChoice `protobuf_oneof:"parameter_choice"`
	XXX_NoUnkeyedLiteral struct{}                         `json:"-"`
	XXX_unrecognized     []byte                           `json:"-"`
	XXX_sizecache        int32                            `json:"-"`
}

func (m *InferParameter) Reset()         { *m = InferParameter{} }
func (m *InferParameter) String() string { return proto.CompactTextString(m) }
func (*InferParameter) ProtoMessage()    {}
func (*InferParameter) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{12}
}
func (m *InferParameter) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferParameter.Unmarshal(m, b)
}
func (m *InferParameter) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferParameter.Marshal(b, m, deterministic)
}
func (m *InferParameter) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferParameter.Merge(m, src)
}
func (m *InferParameter) XXX_Size() int {
	return xxx_messageInfo_InferParameter.Size(m)
}
func (m *InferParameter) XXX_DiscardUnknown() {
	xxx_messageInfo_InferParameter.DiscardUnknown(m)
}

var xxx_messageInfo_InferParameter proto.InternalMessageInfo

type isInferParameter_ParameterChoice interface {
	isInferParameter_ParameterChoice()
}

type InferParameter_BoolParam struct {
	BoolParam bool `protobuf:"varint,1,opt,name=bool_param,json=boolParam,proto3,oneof"`
}
type InferParameter_Int64Param struct {
	Int64Param int64 `protobuf:"varint,2,opt,name=int64_param,json=int64Param,proto3,oneof"`
}
type InferParameter_StringParam struct {
	StringParam string `protobuf:"bytes,3,opt,name=string_param,json=stringParam,proto3,oneof"`
}

func (*InferParameter_BoolParam) isInferParameter_ParameterChoice()   {}
func (*InferParameter_Int64Param) isInferParameter_ParameterChoice()  {}
func (*InferParameter_StringParam) isInferParameter_ParameterChoice() {}

func (m *InferParameter) GetParameterChoice() isInferParameter_ParameterChoice {
	if m != nil {
		return m.ParameterChoice
	}
	return nil
}

func (m *InferParameter) GetBoolParam() bool {
	if x, ok := m.GetParameterChoice().(*InferParameter_BoolParam); ok {
		return x.BoolParam
	}
	return false
}

func (m *InferParameter) GetInt64Param() int64 {
	if x, ok := m.GetParameterChoice().(*InferParameter_Int64Param); ok {
		return x.Int64Param
	}
	return 0
}

func (m *InferParameter) GetStringParam() string {
	if x, ok := m.GetParameterChoice().(*InferParameter_StringParam); ok {
		return x.StringParam
	}
	return ""
}

// XXX_OneofFuncs is for the internal use of the proto package.
func (*InferParameter) XXX_OneofFuncs() (func(msg proto.Message, b *proto.Buffer) error, func(msg proto.Message, tag, wire int, b *proto.Buffer) (bool, error), func(msg proto.Message) (n int), []interface{}) {
	return _InferParameter_OneofMarshaler, _InferParameter_OneofUnmarshaler, _InferParameter_OneofSizer, []interface{}{
		(*InferParameter_BoolParam)(nil),
		(*InferParameter_Int64Param)(nil),
		(*InferParameter_StringParam)(nil),
	}
}

func _InferParameter_OneofMarshaler(msg proto.Message, b *proto.Buffer) error {
	m := msg.(*InferParameter)
	// parameter_choice
	switch x := m.ParameterChoice.(type) {
	case *InferParameter_BoolParam:
		t := uint64(0)
		if x.BoolParam {
			t = 1
		}
		_ = b.EncodeVarint(1<<3 | proto.WireVarint)
		_ = b.EncodeVarint(t)
	case *InferParameter_Int64Param:
		_ = b.EncodeVarint(2<<3 | proto.WireVarint)
		_ = b.EncodeVarint(uint64(x.Int64Param))
	case *InferParameter_StringParam:
		_ = b.EncodeVarint(3<<3 | proto.WireBytes)
		_ = b.EncodeStringBytes(x.StringParam)
	case nil:
	default:
		return fmt.Errorf("InferParameter.ParameterChoice has unexpected type %T", x)
	}
	return nil
}

func _InferParameter_OneofUnmarshaler(msg proto.Message, tag, wire int, b *proto.Buffer) (bool, error) {
	m := msg.(*InferParameter)
	switch tag {
	case 1: // parameter_choice.bool_param
		if wire != proto.WireVarint {
			return true, proto.ErrInternalBadWireType
		}
		x, err := b.DecodeVarint()
		m.ParameterChoice = &InferParameter_BoolParam{x != 0}
		return true, err
	case 2: // parameter_choice.int64_param
		if wire != proto.WireVarint {
			return true, proto.ErrInternalBadWireType
		}
		x, err := b.DecodeVarint()
		m.ParameterChoice = &InferParameter_Int64Param{int64(x)}
		return true, err
	case 3: // parameter_choice.string_param
		if wire != proto.WireBytes {
			return true, proto.ErrInternalBadWireType
		}
		x, err := b.DecodeStringBytes()
		m.ParameterChoice = &InferParameter_StringParam{x}
		return true, err
	default:
		return false, nil
	}
}

func _InferParameter_OneofSizer(msg proto.Message) (n int) {
	m := msg.(*InferParameter)
	// parameter_choice
	switch x := m.ParameterChoice.(type) {
	case *InferParameter_BoolParam:
		n += 1 // tag and wire
		n += 1
	case *InferParameter_Int64Param:
		n += 1 // tag and wire
		n += proto.SizeVarint(uint64(x.Int64Param))
	case *InferParameter_StringParam:
		n += 1 // tag and wire
		n += proto.SizeVarint(uint64(len(x.StringParam)))
		n += len(x.StringParam)
	case nil:
	default:
		panic(fmt.Sprintf("proto: unexpected type %T in oneof", x))
	}
	return n
}

// The data contained in a tensor represented by the repeated type
// that matches the tensor's data type. Protobuf oneof is not used
// because oneofs cannot contain repeated fields.
type InferTensorContents struct {
	// Representation for BOOL data type. The size must match what is
	// expected by the tensor's shape. The contents must be the flattened,
	// one-dimensional, row-major order of the tensor elements.
	BoolContents []bool `protobuf:"varint,1,rep,packed,name=bool_contents,json=boolContents,proto3" json:"bool_contents,omitempty"`
	// Representation for INT8, INT16, and INT32 data types. The size
	// must match what is expected by the tensor's shape. The contents
	// must be the flattened, one-dimensional, row-major order of the
	// tensor elements.
	IntContents []int32 `protobuf:"varint,2,rep,packed,name=int_contents,json=intContents,proto3" json:"int_contents,omitempty"`
	// Representation for INT64 data types. The size must match what
	// is expected by the tensor's shape. The contents must be the
	// flattened, one-dimensional, row-major order of the tensor elements.
	Int64Contents []int64 `protobuf:"varint,3,rep,packed,name=int64_contents,json=int64Contents,proto3" json:"int64_contents,omitempty"`
	// Representation for UINT8, UINT16, and UINT32 data types. The size
	// must match what is expected by the tensor's shape. The contents
	// must be the flattened, one-dimensional, row-major order of the
	// tensor elements.
	UintContents []uint32 `protobuf:"varint,4,rep,packed,name=uint_contents,json=uintContents,proto3" json:"uint_contents,omitempty"`
	// Representation for UINT64 data types. The size must match what
	// is expected by the tensor's shape. The contents must be the
	// flattened, one-dimensional, row-major order of the tensor elements.
	Uint64Contents []uint64 `protobuf:"varint,5,rep,packed,name=uint64_contents,json=uint64Contents,proto3" json:"uint64_contents,omitempty"`
	// Representation for FP32 data type. The size must match what is
	// expected by the tensor's shape. The contents must be the flattened,
	// one-dimensional, row-major order of the tensor elements.
	Fp32Contents []float32 `protobuf:"fixed32,6,rep,packed,name=fp32_contents,json=fp32Contents,proto3" json:"fp32_contents,omitempty"`
	// Representation for FP64 data type. The size must match what is
	// expected by the tensor's shape. The contents must be the flattened,
	// one-dimensional, row-major order of the tensor elements.
	Fp64Contents []float64 `protobuf:"fixed64,7,rep,packed,name=fp64_contents,json=fp64Contents,proto3" json:"fp64_contents,omitempty"`
	// Representation for BYTES data type. The size must match what is
	// expected by the tensor's shape. The contents must be the flattened,
	// one-dimensional, row-major order of the tensor elements.
	BytesContents        [][]byte `protobuf:"bytes,8,rep,name=bytes_contents,json=bytesContents,proto3" json:"bytes_contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *InferTensorContents) Reset()         { *m = InferTensorContents{} }
func (m *InferTensorContents) String() string { return proto.CompactTextString(m) }
func (*InferTensorContents) ProtoMessage()    {}
func (*InferTensorContents) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{13}
}
func (m *InferTensorContents) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferTensorContents.Unmarshal(m, b)
}
func (m *InferTensorContents) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferTensorContents.Marshal(b, m, deterministic)
}
func (m *InferTensorContents) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferTensorContents.Merge(m, src)
}
func (m *InferTensorContents) XXX_Size() int {
	return xxx_messageInfo_InferTensorContents.Size(m)
}
func (m *InferTensorContents) XXX_DiscardUnknown() {
	xxx_messageInfo_InferTensorContents.DiscardUnknown(m)
}

var xxx_messageInfo_InferTensorContents proto.InternalMessageInfo

func (m *InferTensorContents) GetBoolContents() []bool {
	if m != nil {
		return m.BoolContents
	}
	return nil
}

func (m *InferTensorContents) GetIntContents() []int32 {
	if m != nil {
		return m.IntContents
	}
	return nil
}

func (m *InferTensorContents) GetInt64Contents() []int64 {
	if m != nil {
		return m.Int64Contents
	}
	return nil
}

func (m *InferTensorContents) GetUintContents() []uint32 {
	if m != nil {
		return m.UintContents
	}
	return nil
}

func (m *InferTensorContents) GetUint64Contents() []uint64 {
	if m != nil {
		return m.Uint64Contents
	}
	return nil
}

func (m *InferTensorContents) GetFp32Contents() []float32 {
	if m != nil {
		return m.Fp32Contents
	}
	return nil
}

func (m *InferTensorContents) GetFp64Contents() []float64 {
	if m != nil {
		return m.Fp64Contents
	}
	return nil
}

func (m *InferTensorContents) GetBytesContents() [][]byte {
	if m != nil {
		return m.BytesContents
	}
	return nil
}

type RepositoryModelLoadRequest struct {
	// The name of the model to load, or reload.
	ModelName            string   `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryModelLoadRequest) Reset()         { *m = RepositoryModelLoadRequest{} }
func (m *RepositoryModelLoadRequest) String() string { return proto.CompactTextString(m) }
func (*RepositoryModelLoadRequest) ProtoMessage()    {}
func (*RepositoryModelLoadRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{14}
}
func (m *RepositoryModelLoadRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryModelLoadRequest.Unmarshal(m, b)
}
func (m *RepositoryModelLoadRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryModelLoadRequest.Marshal(b, m, deterministic)
}
func (m *RepositoryModelLoadRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryModelLoadRequest.Merge(m, src)
}
func (m *RepositoryModelLoadRequest) XXX_Size() int {
	return xxx_messageInfo_RepositoryModelLoadRequest.Size(m)
}
func (m *RepositoryModelLoadRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryModelLoadRequest.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryModelLoadRequest proto.InternalMessageInfo

func (m *RepositoryModelLoadRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

type RepositoryModelLoadResponse struct {
	// The name of the model trying to load or reload.
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// boolean parameter to indicate whether model is loaded or not
	IsLoaded             bool     `protobuf:"varint,2,opt,name=isLoaded,proto3" json:"isLoaded,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryModelLoadResponse) Reset()         { *m = RepositoryModelLoadResponse{} }
func (m *RepositoryModelLoadResponse) String() string { return proto.CompactTextString(m) }
func (*RepositoryModelLoadResponse) ProtoMessage()    {}
func (*RepositoryModelLoadResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{15}
}
func (m *RepositoryModelLoadResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryModelLoadResponse.Unmarshal(m, b)
}
func (m *RepositoryModelLoadResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryModelLoadResponse.Marshal(b, m, deterministic)
}
func (m *RepositoryModelLoadResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryModelLoadResponse.Merge(m, src)
}
func (m *RepositoryModelLoadResponse) XXX_Size() int {
	return xxx_messageInfo_RepositoryModelLoadResponse.Size(m)
}
func (m *RepositoryModelLoadResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryModelLoadResponse.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryModelLoadResponse proto.InternalMessageInfo

func (m *RepositoryModelLoadResponse) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *RepositoryModelLoadResponse) GetIsLoaded() bool {
	if m != nil {
		return m.IsLoaded
	}
	return false
}

type RepositoryModelUnloadRequest struct {
	// The name of the model to unload.
	ModelName            string   `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryModelUnloadRequest) Reset()         { *m = RepositoryModelUnloadRequest{} }
func (m *RepositoryModelUnloadRequest) String() string { return proto.CompactTextString(m) }
func (*RepositoryModelUnloadRequest) ProtoMessage()    {}
func (*RepositoryModelUnloadRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{16}
}
func (m *RepositoryModelUnloadRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryModelUnloadRequest.Unmarshal(m, b)
}
func (m *RepositoryModelUnloadRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryModelUnloadRequest.Marshal(b, m, deterministic)
}
func (m *RepositoryModelUnloadRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryModelUnloadRequest.Merge(m, src)
}
func (m *RepositoryModelUnloadRequest) XXX_Size() int {
	return xxx_messageInfo_RepositoryModelUnloadRequest.Size(m)
}
func (m *RepositoryModelUnloadRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryModelUnloadRequest.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryModelUnloadRequest proto.InternalMessageInfo

func (m *RepositoryModelUnloadRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

type RepositoryModelUnloadResponse struct {
	// The name of the model trying to load or reload.
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// boolean parameter to indicate whether model is unloaded or not
	IsUnloaded           bool     `protobuf:"varint,2,opt,name=isUnloaded,proto3" json:"isUnloaded,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryModelUnloadResponse) Reset()         { *m = RepositoryModelUnloadResponse{} }
func (m *RepositoryModelUnloadResponse) String() string { return proto.CompactTextString(m) }
func (*RepositoryModelUnloadResponse) ProtoMessage()    {}
func (*RepositoryModelUnloadResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_fa28bb2b67057145, []int{17}
}
func (m *RepositoryModelUnloadResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryModelUnloadResponse.Unmarshal(m, b)
}
func (m *RepositoryModelUnloadResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryModelUnloadResponse.Marshal(b, m, deterministic)
}
func (m *RepositoryModelUnloadResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryModelUnloadResponse.Merge(m, src)
}
func (m *RepositoryModelUnloadResponse) XXX_Size() int {
	return xxx_messageInfo_RepositoryModelUnloadResponse.Size(m)
}
func (m *RepositoryModelUnloadResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryModelUnloadResponse.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryModelUnloadResponse proto.InternalMessageInfo

func (m *RepositoryModelUnloadResponse) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *RepositoryModelUnloadResponse) GetIsUnloaded() bool {
	if m != nil {
		return m.IsUnloaded
	}
	return false
}

func init() {
	proto.RegisterType((*ServerLiveRequest)(nil), "inference.ServerLiveRequest")
	proto.RegisterType((*ServerLiveResponse)(nil), "inference.ServerLiveResponse")
	proto.RegisterType((*ServerReadyRequest)(nil), "inference.ServerReadyRequest")
	proto.RegisterType((*ServerReadyResponse)(nil), "inference.ServerReadyResponse")
	proto.RegisterType((*ModelReadyRequest)(nil), "inference.ModelReadyRequest")
	proto.RegisterType((*ModelReadyResponse)(nil), "inference.ModelReadyResponse")
	proto.RegisterType((*ServerMetadataRequest)(nil), "inference.ServerMetadataRequest")
	proto.RegisterType((*ServerMetadataResponse)(nil), "inference.ServerMetadataResponse")
	proto.RegisterType((*ModelMetadataRequest)(nil), "inference.ModelMetadataRequest")
	proto.RegisterType((*ModelMetadataResponse)(nil), "inference.ModelMetadataResponse")
	proto.RegisterType((*ModelMetadataResponse_TensorMetadata)(nil), "inference.ModelMetadataResponse.TensorMetadata")
	proto.RegisterType((*ModelInferRequest)(nil), "inference.ModelInferRequest")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferRequest.ParametersEntry")
	proto.RegisterType((*ModelInferRequest_InferInputTensor)(nil), "inference.ModelInferRequest.InferInputTensor")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferRequest.InferInputTensor.ParametersEntry")
	proto.RegisterType((*ModelInferRequest_InferRequestedOutputTensor)(nil), "inference.ModelInferRequest.InferRequestedOutputTensor")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry")
	proto.RegisterType((*ModelInferResponse)(nil), "inference.ModelInferResponse")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferResponse.ParametersEntry")
	proto.RegisterType((*ModelInferResponse_InferOutputTensor)(nil), "inference.ModelInferResponse.InferOutputTensor")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferResponse.InferOutputTensor.ParametersEntry")
	proto.RegisterType((*InferParameter)(nil), "inference.InferParameter")
	proto.RegisterType((*InferTensorContents)(nil), "inference.InferTensorContents")
	proto.RegisterType((*RepositoryModelLoadRequest)(nil), "inference.RepositoryModelLoadRequest")
	proto.RegisterType((*RepositoryModelLoadResponse)(nil), "inference.RepositoryModelLoadResponse")
	proto.RegisterType((*RepositoryModelUnloadRequest)(nil), "inference.RepositoryModelUnloadRequest")
	proto.RegisterType((*RepositoryModelUnloadResponse)(nil), "inference.RepositoryModelUnloadResponse")
}

func init() {
	proto.RegisterFile("api/inference/v2/grpc_predict_v2.proto", fileDescriptor_fa28bb2b67057145)
}

var fileDescriptor_fa28bb2b67057145 = []byte{
	// 1096 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xd4, 0x57, 0xdd, 0x6e, 0x1b, 0x45,
	0x14, 0xb6, 0xd7, 0x71, 0x62, 0x1f, 0xff, 0x34, 0x19, 0x27, 0x60, 0x96, 0x26, 0x75, 0xb6, 0x4a,
	0x6b, 0x41, 0xb1, 0x25, 0x17, 0x01, 0x2a, 0xaa, 0x10, 0x94, 0x2a, 0x89, 0x9a, 0x94, 0x32, 0x94,
	0xd2, 0x1b, 0xb0, 0x36, 0xf6, 0x38, 0x5d, 0xe1, 0xec, 0x2e, 0xbb, 0x63, 0x07, 0x3f, 0x04, 0x8f,
	0xc0, 0xd3, 0x70, 0x8d, 0x40, 0xbc, 0x00, 0x0f, 0xc1, 0x0b, 0xa0, 0xf9, 0xd9, 0xf1, 0xec, 0x7a,
	0xfd, 0x93, 0xaa, 0xb9, 0xe8, 0x9d, 0xcf, 0x99, 0xef, 0x7c, 0xe7, 0x67, 0xce, 0x19, 0xef, 0x81,
	0x3b, 0xb6, 0xef, 0xb4, 0x1d, 0x77, 0x40, 0x02, 0xe2, 0xf6, 0x48, 0x7b, 0xdc, 0x69, 0x9f, 0x07,
	0x7e, 0xaf, 0xeb, 0x07, 0xa4, 0xef, 0xf4, 0x68, 0x77, 0xdc, 0x69, 0xf9, 0x81, 0x47, 0x3d, 0x54,
	0x54, 0x18, 0xab, 0x06, 0x5b, 0xdf, 0x91, 0x60, 0x4c, 0x82, 0x13, 0x67, 0x4c, 0x30, 0xf9, 0x65,
	0x44, 0x42, 0x6a, 0x35, 0x01, 0xe9, 0xca, 0xd0, 0xf7, 0xdc, 0x90, 0x20, 0x04, 0x6b, 0x43, 0x67,
	0x4c, 0xea, 0xd9, 0x46, 0xb6, 0x59, 0xc0, 0xfc, 0xb7, 0xb5, 0x1d, 0x21, 0x31, 0xb1, 0xfb, 0x93,
	0xc8, 0xfe, 0x43, 0xa8, 0xc5, 0xb4, 0x92, 0x60, 0x1b, 0xf2, 0x01, 0x53, 0x48, 0x06, 0x21, 0x58,
	0x5f, 0xc2, 0xd6, 0xa9, 0xd7, 0x27, 0x43, 0x9d, 0x81, 0xf9, 0x72, 0xed, 0x0b, 0xe1, 0xab, 0x88,
	0xf9, 0x6f, 0x54, 0x87, 0x8d, 0x31, 0x09, 0x42, 0xc7, 0x73, 0xeb, 0x06, 0x57, 0x47, 0xa2, 0xf5,
	0x01, 0x20, 0x9d, 0x62, 0xa1, 0xbb, 0x77, 0x61, 0x47, 0xc4, 0x76, 0x4a, 0xa8, 0xdd, 0xb7, 0xa9,
	0x1d, 0x05, 0x3d, 0x80, 0x77, 0x92, 0x07, 0xd3, 0xc4, 0x57, 0x0f, 0x06, 0xed, 0x01, 0x90, 0x5f,
	0x29, 0x71, 0x99, 0x10, 0xd6, 0x73, 0x8d, 0x5c, 0xb3, 0x88, 0x35, 0x8d, 0xf5, 0x35, 0x6c, 0xf3,
	0x60, 0x13, 0xfe, 0xaf, 0x98, 0xf2, 0x5f, 0x06, 0xec, 0x24, 0x68, 0x16, 0x44, 0x6b, 0x42, 0x41,
	0x1a, 0x86, 0x75, 0x83, 0x47, 0xa4, 0x64, 0x76, 0xe6, 0x0f, 0x6d, 0x3a, 0xf0, 0x82, 0x8b, 0x7a,
	0x8e, 0xdb, 0x28, 0x19, 0x1d, 0xc2, 0xba, 0xe3, 0xfa, 0x23, 0x1a, 0xd6, 0xd7, 0x1a, 0xb9, 0x66,
	0xa9, 0xd3, 0x6e, 0xa9, 0xce, 0x69, 0xa5, 0x7a, 0x6f, 0x3d, 0x27, 0x6e, 0xe8, 0x4d, 0x4b, 0x28,
	0xcd, 0xd1, 0x31, 0x6c, 0x78, 0x23, 0xca, 0x99, 0xf2, 0xaf, 0xc7, 0x14, 0xd9, 0x9b, 0x2f, 0xa0,
	0x1a, 0x3f, 0x9a, 0x97, 0x31, 0x3b, 0xa3, 0x13, 0x9f, 0xc8, 0xd2, 0x29, 0x99, 0x35, 0x46, 0xf8,
	0xca, 0xf6, 0x09, 0xbf, 0x9c, 0x1c, 0x16, 0x82, 0xf5, 0x7b, 0x41, 0x36, 0xe2, 0x31, 0x0b, 0x2c,
	0xba, 0x95, 0x5d, 0x80, 0x0b, 0xa6, 0xec, 0x6a, 0x1e, 0x8a, 0x5c, 0xf3, 0x94, 0xb9, 0xb9, 0x0d,
	0x15, 0x71, 0x1c, 0xbf, 0xa6, 0x32, 0x57, 0xbe, 0x90, 0x1d, 0x51, 0x05, 0xc3, 0xe9, 0xcb, 0xda,
	0x1a, 0x4e, 0x1f, 0x9d, 0x00, 0xf8, 0x76, 0x60, 0x5f, 0x10, 0x4a, 0x82, 0xa8, 0xb2, 0xf7, 0x92,
	0xf5, 0xd0, 0xa3, 0x68, 0x3d, 0x53, 0xf0, 0xc7, 0x2e, 0x0d, 0x26, 0x58, 0xb3, 0x47, 0x8f, 0xd5,
	0x1d, 0x89, 0xca, 0x7e, 0xb4, 0x90, 0x89, 0x0b, 0xc7, 0x0c, 0x2f, 0x8a, 0xa8, 0x6e, 0xe8, 0xdb,
	0xe9, 0x0d, 0xad, 0x73, 0x9e, 0x4f, 0x97, 0xf3, 0x48, 0x81, 0xf4, 0xbf, 0xe1, 0x96, 0x92, 0x31,
	0xe2, 0x41, 0xf7, 0x00, 0x05, 0xf6, 0x65, 0x97, 0x3b, 0xe8, 0xf6, 0x3c, 0x97, 0x12, 0x97, 0x86,
	0xf5, 0x8d, 0x46, 0xae, 0x59, 0xc6, 0x9b, 0x81, 0x7d, 0xc9, 0xc3, 0x78, 0x24, 0xf5, 0xe6, 0x9f,
	0x06, 0x6c, 0x26, 0xa3, 0x7b, 0x33, 0x57, 0x8b, 0x7e, 0x4c, 0x29, 0xf8, 0xc3, 0x2b, 0x95, 0x69,
	0xe1, 0x0d, 0x3c, 0x80, 0x82, 0xca, 0x2e, 0xdf, 0xc8, 0x36, 0x4b, 0x9d, 0x3d, 0x8d, 0x9c, 0x53,
	0x09, 0x96, 0x28, 0x57, 0xac, 0xf0, 0xe6, 0x4b, 0xb8, 0x91, 0xa0, 0x46, 0x9b, 0x90, 0xfb, 0x99,
	0x4c, 0x64, 0xca, 0xec, 0x27, 0x6a, 0x43, 0x7e, 0x6c, 0x0f, 0x47, 0x22, 0xdd, 0x52, 0xe7, 0xbd,
	0x24, 0xbb, 0x62, 0xc0, 0x02, 0xf7, 0xc0, 0xf8, 0x2c, 0x6b, 0xfe, 0x97, 0x05, 0x73, 0xfe, 0x2d,
	0xa5, 0x56, 0xf6, 0x3c, 0x56, 0x27, 0x83, 0xd7, 0xe9, 0xf0, 0x35, 0xdb, 0x60, 0x51, 0xc5, 0xae,
	0x31, 0xeb, 0x6b, 0x63, 0xb6, 0xfe, 0xcd, 0xcb, 0x7f, 0x19, 0x99, 0xb3, 0x7c, 0x6e, 0xaf, 0xe3,
	0x81, 0x38, 0x4d, 0xe9, 0xd7, 0x79, 0x63, 0x2d, 0x5f, 0xcb, 0x45, 0xfd, 0xb9, 0xfc, 0xf1, 0x8d,
	0x73, 0x71, 0x29, 0x7d, 0xa4, 0x5b, 0x50, 0x63, 0x23, 0x2d, 0xc4, 0xe9, 0x4c, 0xaf, 0xf3, 0x99,
	0xde, 0x0a, 0xec, 0x4b, 0x61, 0xa6, 0x86, 0xfa, 0x6f, 0x03, 0xb6, 0x66, 0xe8, 0xde, 0xd0, 0x54,
	0x77, 0x53, 0xaa, 0xf4, 0xc5, 0x15, 0x33, 0x7b, 0x0b, 0xe7, 0xfa, 0xfa, 0x3a, 0xfc, 0xb7, 0x2c,
	0x54, 0xe3, 0xa7, 0xe8, 0x16, 0xc0, 0x99, 0xe7, 0x0d, 0xbb, 0xbc, 0x2a, 0xe2, 0x43, 0xea, 0x28,
	0x83, 0x8b, 0x4c, 0xc7, 0x41, 0x68, 0x1f, 0x4a, 0x8e, 0x4b, 0x3f, 0xf9, 0x58, 0x22, 0x98, 0xbb,
	0xdc, 0x51, 0x06, 0x03, 0x57, 0x0a, 0xc8, 0x6d, 0x28, 0x87, 0x34, 0x70, 0xdc, 0x73, 0x89, 0xe1,
	0x7d, 0x7e, 0x94, 0xc1, 0x25, 0xa1, 0xe5, 0xa0, 0xaf, 0x10, 0x6c, 0xaa, 0xca, 0x77, 0x7b, 0xaf,
	0x3c, 0xa7, 0x47, 0xac, 0x3f, 0x0c, 0xa8, 0xa5, 0x54, 0x99, 0xcd, 0x14, 0x0f, 0x4a, 0x5d, 0x4e,
	0xb6, 0x91, 0x6b, 0x16, 0x70, 0x99, 0x29, 0x15, 0x68, 0x1f, 0xca, 0x8e, 0xab, 0xb5, 0x28, 0x7b,
	0xcd, 0xf2, 0x98, 0x05, 0xab, 0x20, 0x07, 0x50, 0x15, 0xb1, 0x2b, 0x90, 0xe8, 0xaf, 0x0a, 0xd7,
	0xea, 0xee, 0x46, 0x31, 0x2a, 0xd6, 0x6a, 0x15, 0x5c, 0x1e, 0xe9, 0x5c, 0x77, 0xe1, 0xc6, 0x28,
	0x41, 0xc6, 0x66, 0x6d, 0x0d, 0x57, 0x47, 0x33, 0x6c, 0x03, 0xff, 0x7e, 0x27, 0x3e, 0x3b, 0x06,
	0x2e, 0x33, 0x65, 0x1c, 0xa4, 0x73, 0xb1, 0x3f, 0xcd, 0x2c, 0x03, 0x69, 0x4c, 0x07, 0x50, 0x3d,
	0x9b, 0x50, 0x12, 0x4e, 0x51, 0x05, 0x3e, 0x86, 0x15, 0xae, 0x8d, 0x60, 0xd6, 0xe7, 0x60, 0x62,
	0xe2, 0x7b, 0xa1, 0x43, 0xbd, 0x60, 0xc2, 0x67, 0xe2, 0xc4, 0xb3, 0xfb, 0xab, 0x7d, 0xdf, 0x58,
	0x2f, 0xe1, 0xfd, 0x54, 0xe3, 0xd5, 0x1e, 0x3f, 0x13, 0x0a, 0x4e, 0xc8, 0x0c, 0x48, 0x9f, 0x77,
	0x46, 0x01, 0x2b, 0xd9, 0x7a, 0x08, 0x37, 0x13, 0xcc, 0xdf, 0xbb, 0xc3, 0xd5, 0x03, 0xfb, 0x09,
	0x76, 0xe7, 0x98, 0xaf, 0x16, 0xda, 0x1e, 0x80, 0x13, 0x0a, 0x13, 0x15, 0x9c, 0xa6, 0xe9, 0xfc,
	0x93, 0x87, 0xed, 0x43, 0xfc, 0xec, 0xd1, 0x71, 0x34, 0x37, 0x6c, 0x37, 0x70, 0x7a, 0x04, 0x3d,
	0x01, 0x98, 0xee, 0x46, 0xe8, 0xa6, 0x36, 0x58, 0x33, 0x7b, 0x94, 0xb9, 0x3b, 0xe7, 0x54, 0x84,
	0x68, 0x65, 0xd0, 0x53, 0x28, 0x69, 0x8b, 0x12, 0x9a, 0xc5, 0xeb, 0x4b, 0x91, 0xb9, 0x37, 0xef,
	0x58, 0xf1, 0x3d, 0x01, 0x98, 0x2e, 0x42, 0xb1, 0xe0, 0x66, 0x56, 0xac, 0x58, 0x70, 0xb3, 0xdb,
	0x93, 0x95, 0x41, 0x3f, 0x40, 0x35, 0xbe, 0x10, 0xa1, 0xc6, 0x4c, 0x00, 0x89, 0x25, 0xc6, 0xdc,
	0x5f, 0x80, 0x50, 0xc4, 0xcf, 0xa1, 0x12, 0xfb, 0xe4, 0x47, 0xb7, 0xe6, 0x2f, 0x03, 0x82, 0xb6,
	0xb1, 0x6c, 0x5b, 0xd0, 0x72, 0xe7, 0x37, 0x36, 0x9b, 0xbb, 0xfe, 0xa5, 0x32, 0x9b, 0x7b, 0xec,
	0x6f, 0xc2, 0xca, 0xa0, 0x01, 0xd4, 0x52, 0xfa, 0x1e, 0x1d, 0x68, 0x76, 0xf3, 0x87, 0xca, 0xbc,
	0xb3, 0x0c, 0xa6, 0xfc, 0x0c, 0x61, 0x27, 0xb5, 0x8d, 0xd1, 0xdd, 0xf9, 0x14, 0xb1, 0x39, 0x31,
	0x9b, 0xcb, 0x81, 0x91, 0xb7, 0xb3, 0x75, 0xbe, 0xfe, 0xdf, 0xff, 0x3f, 0x00, 0x00, 0xff, 0xff,
	0x15, 0x66, 0xb8, 0xa9, 0x28, 0x10, 0x00, 0x00,
}

// Reference imports to suppress errors if they are not otherwise used.
var _ context.Context
var _ grpc.ClientConn

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
const _ = grpc.SupportPackageIsVersion4

// GRPCInferenceServiceClient is the client API for GRPCInferenceService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.
type GRPCInferenceServiceClient interface {
	// The ServerLive API indicates if the inference server is able to receive
	// and respond to metadata and inference requests.
	ServerLive(ctx context.Context, in *ServerLiveRequest, opts ...grpc.CallOption) (*ServerLiveResponse, error)
	// The ServerReady API indicates if the server is ready for inferencing.
	ServerReady(ctx context.Context, in *ServerReadyRequest, opts ...grpc.CallOption) (*ServerReadyResponse, error)
	// The ModelReady API indicates if a specific model is ready for inferencing.
	ModelReady(ctx context.Context, in *ModelReadyRequest, opts ...grpc.CallOption) (*ModelReadyResponse, error)
	// The ServerMetadata API provides information about the server. Errors are
	// indicated by the google.rpc.Status returned for the request. The OK code
	// indicates success and other codes indicate failure.
	ServerMetadata(ctx context.Context, in *ServerMetadataRequest, opts ...grpc.CallOption) (*ServerMetadataResponse, error)
	// The per-model metadata API provides information about a model. Errors are
	// indicated by the google.rpc.Status returned for the request. The OK code
	// indicates success and other codes indicate failure.
	ModelMetadata(ctx context.Context, in *ModelMetadataRequest, opts ...grpc.CallOption) (*ModelMetadataResponse, error)
	// The ModelInfer API performs inference using the specified model. Errors are
	// indicated by the google.rpc.Status returned for the request. The OK code
	// indicates success and other codes indicate failure.
	ModelInfer(ctx context.Context, in *ModelInferRequest, opts ...grpc.CallOption) (*ModelInferResponse, error)
	// Load or reload a model from a repository.
	RepositoryModelLoad(ctx context.Context, in *RepositoryModelLoadRequest, opts ...grpc.CallOption) (*RepositoryModelLoadResponse, error)
	// Unload a model.
	RepositoryModelUnload(ctx context.Context, in *RepositoryModelUnloadRequest, opts ...grpc.CallOption) (*RepositoryModelUnloadResponse, error)
}

type gRPCInferenceServiceClient struct {
	cc *grpc.ClientConn
}

func NewGRPCInferenceServiceClient(cc *grpc.ClientConn) GRPCInferenceServiceClient {
	return &gRPCInferenceServiceClient{cc}
}

func (c *gRPCInferenceServiceClient) ServerLive(ctx context.Context, in *ServerLiveRequest, opts ...grpc.CallOption) (*ServerLiveResponse, error) {
	out := new(ServerLiveResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ServerLive", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ServerReady(ctx context.Context, in *ServerReadyRequest, opts ...grpc.CallOption) (*ServerReadyResponse, error) {
	out := new(ServerReadyResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ServerReady", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ModelReady(ctx context.Context, in *ModelReadyRequest, opts ...grpc.CallOption) (*ModelReadyResponse, error) {
	out := new(ModelReadyResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ModelReady", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ServerMetadata(ctx context.Context, in *ServerMetadataRequest, opts ...grpc.CallOption) (*ServerMetadataResponse, error) {
	out := new(ServerMetadataResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ServerMetadata", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ModelMetadata(ctx context.Context, in *ModelMetadataRequest, opts ...grpc.CallOption) (*ModelMetadataResponse, error) {
	out := new(ModelMetadataResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ModelMetadata", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ModelInfer(ctx context.Context, in *ModelInferRequest, opts ...grpc.CallOption) (*ModelInferResponse, error) {
	out := new(ModelInferResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ModelInfer", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) RepositoryModelLoad(ctx context.Context, in *RepositoryModelLoadRequest, opts ...grpc.CallOption) (*RepositoryModelLoadResponse, error) {
	out := new(RepositoryModelLoadResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/RepositoryModelLoad", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) RepositoryModelUnload(ctx context.Context, in *RepositoryModelUnloadRequest, opts ...grpc.CallOption) (*RepositoryModelUnloadResponse, error) {
	out := new(RepositoryModelUnloadResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/RepositoryModelUnload", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// GRPCInferenceServiceServer is the server API for GRPCInferenceService service.
type GRPCInferenceServiceServer interface {
	// The ServerLive API indicates if the inference server is able to receive
	// and respond to metadata and inference requests.
	ServerLive(context.Context, *ServerLiveRequest) (*ServerLiveResponse, error)
	// The ServerReady API indicates if the server is ready for inferencing.
	ServerReady(context.Context, *ServerReadyRequest) (*ServerReadyResponse, error)
	// The ModelReady API indicates if a specific model is ready for inferencing.
	ModelReady(context.Context, *ModelReadyRequest) (*ModelReadyResponse, error)
	// The ServerMetadata API provides information about the server. Errors are
	// indicated by the google.rpc.Status returned for the request. The OK code
	// indicates success and other codes indicate failure.
	ServerMetadata(context.Context, *ServerMetadataRequest) (*ServerMetadataResponse, error)
	// The per-model metadata API provides information about a model. Errors are
	// indicated by the google.rpc.Status returned for the request. The OK code
	// indicates success and other codes indicate failure.
	ModelMetadata(context.Context, *ModelMetadataRequest) (*ModelMetadataResponse, error)
	// The ModelInfer API performs inference using the specified model. Errors are
	// indicated by the google.rpc.Status returned for the request. The OK code
	// indicates success and other codes indicate failure.
	ModelInfer(context.Context, *ModelInferRequest) (*ModelInferResponse, error)
	// Load or reload a model from a repository.
	RepositoryModelLoad(context.Context, *RepositoryModelLoadRequest) (*RepositoryModelLoadResponse, error)
	// Unload a model.
	RepositoryModelUnload(context.Context, *RepositoryModelUnloadRequest) (*RepositoryModelUnloadResponse, error)
}

func RegisterGRPCInferenceServiceServer(s *grpc.Server, srv GRPCInferenceServiceServer) {
	s.RegisterService(&_GRPCInferenceService_serviceDesc, srv)
}

func _GRPCInferenceService_ServerLive_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ServerLiveRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ServerLive(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ServerLive",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ServerLive(ctx, req.(*ServerLiveRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ServerReady_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ServerReadyRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ServerReady(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ServerReady",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ServerReady(ctx, req.(*ServerReadyRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ModelReady_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelReadyRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ModelReady(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ModelReady",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ModelReady(ctx, req.(*ModelReadyRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ServerMetadata_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ServerMetadataRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ServerMetadata(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ServerMetadata",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ServerMetadata(ctx, req.(*ServerMetadataRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ModelMetadata_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelMetadataRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ModelMetadata(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ModelMetadata",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ModelMetadata(ctx, req.(*ModelMetadataRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ModelInfer_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelInferRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ModelInfer(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ModelInfer",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ModelInfer(ctx, req.(*ModelInferRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_RepositoryModelLoad_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(RepositoryModelLoadRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).RepositoryModelLoad(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/RepositoryModelLoad",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).RepositoryModelLoad(ctx, req.(*RepositoryModelLoadRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_RepositoryModelUnload_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(RepositoryModelUnloadRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).RepositoryModelUnload(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/RepositoryModelUnload",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).RepositoryModelUnload(ctx, req.(*RepositoryModelUnloadRequest))
	}
	return interceptor(ctx, in, info, handler)
}

var _GRPCInferenceService_serviceDesc = grpc.ServiceDesc{
	ServiceName: "inference.GRPCInferenceService",
	HandlerType: (*GRPCInferenceServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "ServerLive",
			Handler:    _GRPCInferenceService_ServerLive_Handler,
		},
		{
			MethodName: "ServerReady",
			Handler:    _GRPCInferenceService_ServerReady_Handler,
		},
		{
			MethodName: "ModelReady",
			Handler:    _GRPCInferenceService_ModelReady_Handler,
		},
		{
			MethodName: "ServerMetadata",
			Handler:    _GRPCInferenceService_ServerMetadata_Handler,
		},
		{
			MethodName: "ModelMetadata",
			Handler:    _GRPCInferenceService_ModelMetadata_Handler,
		},
		{
			MethodName: "ModelInfer",
			Handler:    _GRPCInferenceService_ModelInfer_Handler,
		},
		{
			MethodName: "RepositoryModelLoad",
			Handler:    _GRPCInferenceService_RepositoryModelLoad_Handler,
		},
		{
			MethodName: "RepositoryModelUnload",
			Handler:    _GRPCInferenceService_RepositoryModelUnload_Handler,
		},
	},
	Streams:  []grpc.StreamDesc{},
	Metadata: "api/inference/v2/grpc_predict_v2.proto",
}
